{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:19.760267Z",
     "iopub.status.busy": "2021-02-19T08:46:19.759747Z",
     "iopub.status.idle": "2021-02-19T08:46:41.216335Z",
     "shell.execute_reply": "2021-02-19T08:46:41.215742Z"
    },
    "papermill": {
     "duration": 21.477561,
     "end_time": "2021-02-19T08:46:41.216558",
     "exception": false,
     "start_time": "2021-02-19T08:46:19.738997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "pkg-config is already the newest version (0.29.1-0ubuntu2).\r\n",
      "python-dev is already the newest version (2.7.15~rc1-1).\r\n",
      "python-dev set to manually installed.\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following additional packages will be installed:\r\n",
      "  libavresample-dev libpostproc-dev\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libavcodec-dev libavdevice-dev libavfilter-dev libavformat-dev\r\n",
      "  libavresample-dev libavutil-dev libpostproc-dev libswresample-dev\r\n",
      "  libswscale-dev\r\n",
      "0 upgraded, 9 newly installed, 0 to remove and 21 not upgraded.\r\n",
      "Need to get 7956 kB of archives.\r\n",
      "After this operation, 33.5 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libavutil-dev amd64 7:3.4.8-0ubuntu0.2 [294 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libswresample-dev amd64 7:3.4.8-0ubuntu0.2 [68.7 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libavcodec-dev amd64 7:3.4.8-0ubuntu0.2 [5079 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libavformat-dev amd64 7:3.4.8-0ubuntu0.2 [1132 kB]\r\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libavresample-dev amd64 7:3.4.8-0ubuntu0.2 [62.0 kB]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libpostproc-dev amd64 7:3.4.8-0ubuntu0.2 [51.0 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libswscale-dev amd64 7:3.4.8-0ubuntu0.2 [166 kB]\r\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libavfilter-dev amd64 7:3.4.8-0ubuntu0.2 [1016 kB]\r\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libavdevice-dev amd64 7:3.4.8-0ubuntu0.2 [87.2 kB]\r\n",
      "Fetched 7956 kB in 0s (17.3 MB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package libavutil-dev:amd64.\r\n",
      "(Reading database ... 116896 files and directories currently installed.)\r\n",
      "Preparing to unpack .../0-libavutil-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libavutil-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libswresample-dev:amd64.\r\n",
      "Preparing to unpack .../1-libswresample-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libswresample-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libavcodec-dev:amd64.\r\n",
      "Preparing to unpack .../2-libavcodec-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libavcodec-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libavformat-dev:amd64.\r\n",
      "Preparing to unpack .../3-libavformat-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libavformat-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libavresample-dev:amd64.\r\n",
      "Preparing to unpack .../4-libavresample-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libavresample-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libpostproc-dev:amd64.\r\n",
      "Preparing to unpack .../5-libpostproc-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libpostproc-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libswscale-dev:amd64.\r\n",
      "Preparing to unpack .../6-libswscale-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libswscale-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libavfilter-dev:amd64.\r\n",
      "Preparing to unpack .../7-libavfilter-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libavfilter-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Selecting previously unselected package libavdevice-dev:amd64.\r\n",
      "Preparing to unpack .../8-libavdevice-dev_7%3a3.4.8-0ubuntu0.2_amd64.deb ...\r\n",
      "Unpacking libavdevice-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libavutil-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libavresample-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libpostproc-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libswscale-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libswresample-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libavcodec-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libavformat-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libavfilter-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Setting up libavdevice-dev:amd64 (7:3.4.8-0ubuntu0.2) ...\r\n",
      "Collecting av\r\n",
      "  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 37.2 MB 141 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: av\r\n",
      "Successfully installed av-8.0.3\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install torchvision for kaggle\n",
    "!apt-get install -y python-dev pkg-config\n",
    "!apt-get install -y libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev\n",
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:41.334694Z",
     "iopub.status.busy": "2021-02-19T08:46:41.334132Z",
     "iopub.status.idle": "2021-02-19T08:46:43.105233Z",
     "shell.execute_reply": "2021-02-19T08:46:43.104762Z"
    },
    "papermill": {
     "duration": 1.831594,
     "end_time": "2021-02-19T08:46:43.105393",
     "exception": false,
     "start_time": "2021-02-19T08:46:41.273799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/drowsiness-batch-dataset-fold1-1/Fold_1_1.csv\n",
      "/kaggle/input/drowsiness-batch-dataset-fold1-1/__results__.html\n",
      "/kaggle/input/drowsiness-batch-dataset-fold1-1/__notebook__.ipynb\n",
      "/kaggle/input/drowsiness-batch-dataset-fold1-1/__output__.json\n",
      "/kaggle/input/drowsiness-batch-dataset-fold1-1/custom.css\n",
      "/kaggle/input/facealignmentmodels/Fold_1_1.csv\n",
      "/kaggle/input/facealignmentmodels/shape_predictor_68_face_landmarks.dat\n",
      "/kaggle/input/facealignmentmodels/test_video2.mp4\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/__results__.html\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/__notebook__.ipynb\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/__output__.json\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/custom.css\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/05/10.MOV\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/05/0.MOV\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/05/5.MOV\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/06/5.mp4\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/06/10.mp4\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/06/0.mp4\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/02/10.MOV\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/02/0.mov\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/02/5.MOV\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/04/5.mp4\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/04/10.mp4\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/04/0.mp4\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/03/10.mov\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/03/0.MOV\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/03/5.mov\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/01/10.MOV\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/01/0.mov\n",
      "/kaggle/input/drowsiness-dataset-fold1-1/Fold1_part1/01/5.mov\n",
      "/kaggle/input/meta-df-pickle/meta_df.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:43.232557Z",
     "iopub.status.busy": "2021-02-19T08:46:43.231818Z",
     "iopub.status.idle": "2021-02-19T08:46:43.435208Z",
     "shell.execute_reply": "2021-02-19T08:46:43.434708Z"
    },
    "papermill": {
     "duration": 0.262183,
     "end_time": "2021-02-19T08:46:43.435330",
     "exception": false,
     "start_time": "2021-02-19T08:46:43.173147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:43.552498Z",
     "iopub.status.busy": "2021-02-19T08:46:43.551900Z",
     "iopub.status.idle": "2021-02-19T08:46:43.828430Z",
     "shell.execute_reply": "2021-02-19T08:46:43.827930Z"
    },
    "papermill": {
     "duration": 0.337825,
     "end_time": "2021-02-19T08:46:43.828572",
     "exception": false,
     "start_time": "2021-02-19T08:46:43.490747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_video_meta(video_path):\n",
    "    cap=cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    duration = frame_count/fps\n",
    "    return {'fps':fps,'frame_count':frame_count, 'duration':duration }\n",
    "\n",
    "\n",
    "def get_path(x):\n",
    "    files = os.listdir(x)\n",
    "    return list(map(lambda file: os.path.join(x, file), files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:43.943945Z",
     "iopub.status.busy": "2021-02-19T08:46:43.943344Z",
     "iopub.status.idle": "2021-02-19T08:46:47.531280Z",
     "shell.execute_reply": "2021-02-19T08:46:47.529979Z"
    },
    "papermill": {
     "duration": 3.64814,
     "end_time": "2021-02-19T08:46:47.531482",
     "exception": false,
     "start_time": "2021-02-19T08:46:43.883342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_path = '../input/drowsiness-batch-dataset-fold1-1/Fold_1_1.csv'\n",
    "df = pd.read_csv(df_path, index_col = 0)\n",
    "meta_path = '../input/meta-df-pickle/meta_df.pkl'\n",
    "meta = pd.read_pickle(meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:47.738708Z",
     "iopub.status.busy": "2021-02-19T08:46:47.737779Z",
     "iopub.status.idle": "2021-02-19T08:46:47.741744Z",
     "shell.execute_reply": "2021-02-19T08:46:47.741238Z"
    },
    "papermill": {
     "duration": 0.11833,
     "end_time": "2021-02-19T08:46:47.741874",
     "exception": false,
     "start_time": "2021-02-19T08:46:47.623544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_files = [\n",
    "    '../input/drowsiness-dataset-fold1-1/Fold1_part1/06/0.mp4',\n",
    "    '../input/drowsiness-dataset-fold1-1/Fold1_part1/06/10.mp4',\n",
    "    '../input/drowsiness-dataset-fold1-1/Fold1_part1/06/5.mp4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:47.931221Z",
     "iopub.status.busy": "2021-02-19T08:46:47.930544Z",
     "iopub.status.idle": "2021-02-19T08:46:47.959925Z",
     "shell.execute_reply": "2021-02-19T08:46:47.960944Z"
    },
    "papermill": {
     "duration": 0.124243,
     "end_time": "2021-02-19T08:46:47.961136",
     "exception": false,
     "start_time": "2021-02-19T08:46:47.836893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>frame</th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>l_18_x</th>\n",
       "      <th>l_18_y</th>\n",
       "      <th>...</th>\n",
       "      <th>l_63_x</th>\n",
       "      <th>l_63_y</th>\n",
       "      <th>l_64_x</th>\n",
       "      <th>l_64_y</th>\n",
       "      <th>l_65_x</th>\n",
       "      <th>l_65_y</th>\n",
       "      <th>l_66_x</th>\n",
       "      <th>l_66_y</th>\n",
       "      <th>l_67_x</th>\n",
       "      <th>l_67_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/drowsiness-dataset-fold1-1/Fold1_part...</td>\n",
       "      <td>10</td>\n",
       "      <td>267</td>\n",
       "      <td>439</td>\n",
       "      <td>577</td>\n",
       "      <td>749</td>\n",
       "      <td>312</td>\n",
       "      <td>472</td>\n",
       "      <td>...</td>\n",
       "      <td>421</td>\n",
       "      <td>652</td>\n",
       "      <td>451</td>\n",
       "      <td>657</td>\n",
       "      <td>422</td>\n",
       "      <td>655</td>\n",
       "      <td>407</td>\n",
       "      <td>657</td>\n",
       "      <td>391</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>../input/drowsiness-dataset-fold1-1/Fold1_part...</td>\n",
       "      <td>10</td>\n",
       "      <td>267</td>\n",
       "      <td>439</td>\n",
       "      <td>577</td>\n",
       "      <td>749</td>\n",
       "      <td>312</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>421</td>\n",
       "      <td>651</td>\n",
       "      <td>452</td>\n",
       "      <td>657</td>\n",
       "      <td>421</td>\n",
       "      <td>655</td>\n",
       "      <td>406</td>\n",
       "      <td>657</td>\n",
       "      <td>391</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>../input/drowsiness-dataset-fold1-1/Fold1_part...</td>\n",
       "      <td>10</td>\n",
       "      <td>267</td>\n",
       "      <td>439</td>\n",
       "      <td>577</td>\n",
       "      <td>749</td>\n",
       "      <td>308</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>418</td>\n",
       "      <td>653</td>\n",
       "      <td>450</td>\n",
       "      <td>658</td>\n",
       "      <td>418</td>\n",
       "      <td>657</td>\n",
       "      <td>403</td>\n",
       "      <td>658</td>\n",
       "      <td>388</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>../input/drowsiness-dataset-fold1-1/Fold1_part...</td>\n",
       "      <td>10</td>\n",
       "      <td>267</td>\n",
       "      <td>439</td>\n",
       "      <td>577</td>\n",
       "      <td>749</td>\n",
       "      <td>306</td>\n",
       "      <td>475</td>\n",
       "      <td>...</td>\n",
       "      <td>417</td>\n",
       "      <td>654</td>\n",
       "      <td>448</td>\n",
       "      <td>658</td>\n",
       "      <td>418</td>\n",
       "      <td>656</td>\n",
       "      <td>403</td>\n",
       "      <td>658</td>\n",
       "      <td>388</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>../input/drowsiness-dataset-fold1-1/Fold1_part...</td>\n",
       "      <td>10</td>\n",
       "      <td>267</td>\n",
       "      <td>439</td>\n",
       "      <td>577</td>\n",
       "      <td>749</td>\n",
       "      <td>305</td>\n",
       "      <td>474</td>\n",
       "      <td>...</td>\n",
       "      <td>417</td>\n",
       "      <td>651</td>\n",
       "      <td>448</td>\n",
       "      <td>655</td>\n",
       "      <td>418</td>\n",
       "      <td>654</td>\n",
       "      <td>404</td>\n",
       "      <td>656</td>\n",
       "      <td>389</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  frame                                               file  label  x_min  \\\n",
       "0   0      0  ../input/drowsiness-dataset-fold1-1/Fold1_part...     10    267   \n",
       "1   0      2  ../input/drowsiness-dataset-fold1-1/Fold1_part...     10    267   \n",
       "2   0      4  ../input/drowsiness-dataset-fold1-1/Fold1_part...     10    267   \n",
       "3   0      6  ../input/drowsiness-dataset-fold1-1/Fold1_part...     10    267   \n",
       "4   0      8  ../input/drowsiness-dataset-fold1-1/Fold1_part...     10    267   \n",
       "\n",
       "   y_min  x_max  y_max  l_18_x  l_18_y  ...  l_63_x  l_63_y  l_64_x  l_64_y  \\\n",
       "0    439    577    749     312     472  ...     421     652     451     657   \n",
       "1    439    577    749     312     471  ...     421     651     452     657   \n",
       "2    439    577    749     308     474  ...     418     653     450     658   \n",
       "3    439    577    749     306     475  ...     417     654     448     658   \n",
       "4    439    577    749     305     474  ...     417     651     448     655   \n",
       "\n",
       "   l_65_x  l_65_y  l_66_x  l_66_y  l_67_x  l_67_y  \n",
       "0     422     655     407     657     391     655  \n",
       "1     421     655     406     657     391     655  \n",
       "2     418     657     403     658     388     657  \n",
       "3     418     656     403     658     388     656  \n",
       "4     418     654     404     656     389     655  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:48.203234Z",
     "iopub.status.busy": "2021-02-19T08:46:48.202495Z",
     "iopub.status.idle": "2021-02-19T08:46:48.366805Z",
     "shell.execute_reply": "2021-02-19T08:46:48.367565Z"
    },
    "papermill": {
     "duration": 0.341596,
     "end_time": "2021-02-19T08:46:48.367722",
     "exception": false,
     "start_time": "2021-02-19T08:46:48.026126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/05/10.MOV\n",
      "56\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/05/0.MOV\n",
      "57\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/05/5.MOV\n",
      "57\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/06/5.mp4\n",
      "38\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/06/10.mp4\n",
      "32\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/06/0.mp4\n",
      "37\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/02/10.MOV\n",
      "60\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/02/0.mov\n",
      "60\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/02/5.MOV\n",
      "60\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/04/5.mp4\n",
      "55\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/04/10.mp4\n",
      "51\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/04/0.mp4\n",
      "48\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/03/10.mov\n",
      "61\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/03/0.MOV\n",
      "47\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/03/5.mov\n",
      "60\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/01/10.MOV\n",
      "59\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/01/0.mov\n",
      "60\n",
      "../input/drowsiness-dataset-fold1-1/Fold1_part1/01/5.mov\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "files = df.file.unique()\n",
    "for file in files :\n",
    "    print(file)\n",
    "    df_f = df[df.file == file]\n",
    "    print(df_f.id.values.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:48.490073Z",
     "iopub.status.busy": "2021-02-19T08:46:48.489155Z",
     "iopub.status.idle": "2021-02-19T08:46:48.514849Z",
     "shell.execute_reply": "2021-02-19T08:46:48.515253Z"
    },
    "papermill": {
     "duration": 0.091261,
     "end_time": "2021-02-19T08:46:48.515429",
     "exception": false,
     "start_time": "2021-02-19T08:46:48.424168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = df[~df.file.isin(test_files)]\n",
    "test_df = df[df.file.isin(test_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:48.658970Z",
     "iopub.status.busy": "2021-02-19T08:46:48.642081Z",
     "iopub.status.idle": "2021-02-19T08:46:48.661495Z",
     "shell.execute_reply": "2021-02-19T08:46:48.661042Z"
    },
    "papermill": {
     "duration": 0.088405,
     "end_time": "2021-02-19T08:46:48.661611",
     "exception": false,
     "start_time": "2021-02-19T08:46:48.573206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VideoFrameDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 annotation_df: pd.DataFrame,\n",
    "                 num_segments: int = 3,\n",
    "                 frames_per_segment: int = 1,\n",
    "                 transform = None,\n",
    "                 e2e: bool = False, #if not e2e then crop face\n",
    "                 jump_frame: int = 1,\n",
    "                 meta_df = None \n",
    "                ):\n",
    "        \"\"\"\n",
    "        Paramters:\n",
    "            annotation_df: DataFrame prepare with the format similar to the dataframe above\n",
    "            num_segments: number of datapoint per clip\n",
    "            frames_per_segment: number of frame use in a datapoint\n",
    "            e2e: \n",
    "                if True data used for end to end model / image is not cropped\n",
    "                if False each frame is cropped to the bounding box of users face\n",
    "            jump_frame: number of index jump in for frames in the index \n",
    "                (e.g: jump_frame=2, dataset[0] will be using clip1 at row with index 0,2,4,6,8 - (in this case frames_per_segment=5)) \n",
    "                (e.g: jump_frame=0, dataset[0] will be using clip1 at row with index 0,1,2,3 - (in this case frames_per_segment=4)) \n",
    "        \n",
    "        \"\"\"\n",
    "        super(VideoFrameDataset, self).__init__()\n",
    "\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.annotation_df_list is contains multiple df\n",
    "            Each dataframe store annotation detail for 1 clip\n",
    "        \"\"\"\n",
    "        annotation_df_list = []\n",
    "        meta_dict_list = []\n",
    "        \n",
    "        self.e2e = e2e\n",
    "#         if not self.e2e:\n",
    "#             annotation_df = resize_keypoints(annotation_df, new_shape = (300,300))\n",
    "      \n",
    "        \n",
    "        for video_path, df in tqdm(annotation_df.groupby('file')):\n",
    "                annotation_df_list.append(df.sort_values(by=['frame']).reset_index(drop=True))\n",
    "        \n",
    "        \n",
    "        if (not meta_df is None):\n",
    "            self.meta_df = meta_df\n",
    "        else:\n",
    "            for video_path, df in tqdm(annotation_df.groupby('file')):\n",
    "                dic = {}\n",
    "                dic['video_path'] = video_path\n",
    "\n",
    "                #TODO: move this to preprocessing step in which time stamp is calculated during df preparation step\n",
    "                time_stamps, fps = torchvision.io.read_video_timestamps(video_path, pts_unit='sec')\n",
    "                temp_time_stamps = time_stamps\n",
    "                dic['time_stamps'] = time_stamps\n",
    "                dic['fps'] = fps\n",
    "                # add dataframe into list variable - reset index of every individual \n",
    "                meta_dict_list.append(dic)\n",
    "                self.meta_df = pd.DataFrame(meta_dict_list)\n",
    "                self.meta_df.to_pickle('./meta_df.pkl')\n",
    "\n",
    "\n",
    "        self.annotation_df_list = annotation_df_list\n",
    "        self.video_num = len(self.annotation_df_list)\n",
    "        self.num_segments = num_segments\n",
    "\n",
    "        \n",
    "        assert frames_per_segment >=1, 'frames_per_segment less then one'\n",
    "        self.frames_per_segment = frames_per_segment            \n",
    "        self.jump_frame = jump_frame\n",
    "        self.landmark_cols = self.get_facial_landmarks(annotation_df) \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.video_num*self.frames_per_segment\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #check index is not larger than lenght of data\n",
    "#         assert index < self.__len__()\n",
    "        row = self.index2row(index)\n",
    "        \n",
    "        # declare variable to use\n",
    "        video_path = row['file'][0]\n",
    "        dic = self.meta_df[self.meta_df['video_path'] == video_path].iloc[0]\n",
    "        \n",
    "        frames = row['frame'].values\n",
    "        fps = dic['fps']\n",
    "        \n",
    "        label = row['label'][0]\n",
    "        label = self.convert_label(label)\n",
    "        landmark_columns = row[self.landmark_cols]\n",
    "        if self.e2e:\n",
    "            frame = elf.read_specific_frame(frames[0], video_path, dic['time_stamps'])\n",
    "        else:\n",
    "            bbox = row[['y_min', 'y_max', 'x_min', 'x_max']].values[0]\n",
    "            frame = self.read_specific_frame(\n",
    "                frames[0], \n",
    "                video_path, \n",
    "                dic['time_stamps'], \n",
    "                face_bbox=bbox\n",
    "            )\n",
    "            landmark_columns = self.convert_facial_landmark_coordinate(row)\n",
    "        ################################\n",
    "        \n",
    "        # Read frame from video and concat them into 1 tensor\n",
    "#         video_tensor = torch.cat(video_tensor)\n",
    "        \n",
    "        return frame[0], label, landmark_columns.values\n",
    "\n",
    "    \n",
    "    \n",
    "    def read_specific_frame(self, frame, video_path, time_stamps, face_bbox=None, transforms=None):\n",
    "        \"\"\"\n",
    "        Return a specific frame in the video clip\n",
    "        \"\"\"\n",
    "        index = time_stamps[frame]\n",
    "        start_pts = end_pts = time_stamps[frame]\n",
    "        vframe, aframe, info = torchvision.io.read_video(\n",
    "            video_path, \n",
    "            start_pts=start_pts, \n",
    "            end_pts=end_pts, \n",
    "            pts_unit='sec'\n",
    "        )\n",
    "        file_type = video_path.split('.')[-1]\n",
    "        if file_type != 'mp4' :\n",
    "            vframe = vframe.permute(0,2,1,3)\n",
    "            vframe = torch.flip(vframe, [2])\n",
    "\n",
    "        if (not face_bbox is None):\n",
    "            y_min, y_max, x_min, x_max = face_bbox\n",
    "            x_min = 0 if x_min < 0 else x_min\n",
    "            y_min = 0 if y_min < 0 else y_min\n",
    "            # video is on the in the correct orientation (inverse orientation)\n",
    "            vframe = vframe[:,y_min:y_max, x_min:x_max, :]\n",
    "            \n",
    "            # resize vframe so that all face have the same size\n",
    "            vframe =  vframe.permute(0,3,1,2)\n",
    "            vframe = F.interpolate(vframe, (300, 300))\n",
    "            vframe = vframe.permute(0,2,3,1)        \n",
    "        return vframe\n",
    "    \n",
    "    \n",
    "    def convert_label(self, input_label):\n",
    "        label_mapping = {\n",
    "            0:0,\n",
    "            5:1,\n",
    "            10:2\n",
    "        }\n",
    "        return label_mapping[input_label]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_facial_landmarks(self, df, combined=True):\n",
    "        facial_landmarks_x, facial_landmarks_y = [], []\n",
    "        for i in df.keys():\n",
    "            if(i.split('_')[0]=='l') and i.split('_')[-1]=='x':\n",
    "                facial_landmarks_x.append(i)\n",
    "            elif (i.split('_')[0]=='l') and i.split('_')[-1]=='y':\n",
    "                facial_landmarks_y.append(i)\n",
    "        if combined:\n",
    "            return facial_landmarks_x + facial_landmarks_y\n",
    "        else:\n",
    "            return facial_landmarks_x, facial_landmarks_y\n",
    "\n",
    "\n",
    "    def get_landmark(self, row, landmark_num):\n",
    "        x,y = row[f\"l_{landmark_num}_x\", f\"l_{landmark_num}_y\"].values\n",
    "        return x,y\n",
    "    \n",
    "    def convert_facial_landmark_coordinate(self, row, new_shape=(300,300)):\n",
    "        df = deepcopy(row)\n",
    "        x_col, y_col = self.get_facial_landmarks(df, combined=False)\n",
    "        x_df = df[x_col].sub(df['x_min'], axis=0)\n",
    "        y_df = df[y_col].sub(df['y_min'], axis=0)\n",
    "        \n",
    "        \n",
    "        face_bbox = df[['x_min', 'y_min', 'x_max', 'y_max']]\n",
    "\n",
    "        y_scale = (face_bbox['y_max'] - face_bbox['y_min']) / new_shape[0]\n",
    "        x_scale = (face_bbox['x_max'] - face_bbox['x_min']) / new_shape[1] \n",
    "        \n",
    "        x_df = x_df.divide(x_scale, axis=0)\n",
    "        y_df = y_df.divide(y_scale, axis=0)\n",
    "        return pd.DataFrame.join(x_df, y_df)\n",
    "\n",
    "        \n",
    "    \n",
    "    def index2row(self, index):\n",
    "        \"\"\"\n",
    "        self.annotation_df_list is contains multiple df\n",
    "            Each dataframe store annotation detail for 1 clip\n",
    "            This function convert index used by pytorch dataset into the rows annotation dataframe\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        video_index = index % self.video_num\n",
    "        df = self.annotation_df_list[video_index]\n",
    "        row_index = index//self.video_num * (len(df) // self.num_segments)\n",
    "        \n",
    "        #careful with case of data point at the end of the video clip        \n",
    "        rows_index = []\n",
    "        for i in range(self.frames_per_segment):\n",
    "            rows_index.append(row_index+(i*self.jump_frame))\n",
    "            # frame jump to too far ahead in video, simply get datapoint as a list of index [-self.frames_per_segment:]\n",
    "            if (df.index.max() < max(rows_index)):\n",
    "                return df.iloc[-self.frames_per_segment:].reset_index(drop=True)\n",
    "        return df.iloc[rows_index].reset_index(drop=True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:48.783949Z",
     "iopub.status.busy": "2021-02-19T08:46:48.783220Z",
     "iopub.status.idle": "2021-02-19T08:46:48.787249Z",
     "shell.execute_reply": "2021-02-19T08:46:48.786799Z"
    },
    "papermill": {
     "duration": 0.069009,
     "end_time": "2021-02-19T08:46:48.787384",
     "exception": false,
     "start_time": "2021-02-19T08:46:48.718375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_facial_landmarks(df, combined=True):\n",
    "        facial_landmarks_x, facial_landmarks_y = [], []\n",
    "        for i in df.keys():\n",
    "            if(i.split('_')[0]=='l') and i.split('_')[-1]=='x':\n",
    "                facial_landmarks_x.append(i)\n",
    "            elif (i.split('_')[0]=='l') and i.split('_')[-1]=='y':\n",
    "                facial_landmarks_y.append(i)\n",
    "        if combined:\n",
    "            return facial_landmarks_x + facial_landmarks_y\n",
    "        else:\n",
    "            return facial_landmarks_x, facial_landmarks_y\n",
    "\n",
    "def resize_keypoints(df, new_shape = (300,300)):\n",
    "    \"\"\"\n",
    "    - Input:\n",
    "        1. Tuple of original data shape\n",
    "        2. Tuple of key_points data\n",
    "        3. Tuple of desired shape.\n",
    "    - Output:\n",
    "        1. A tuple of resized key_points\n",
    "    \"\"\"\n",
    "    facial_landmarks_x, facial_landmarks_y = get_facial_landmarks(df, combined=False)\n",
    "    face_bbox = df[['x_min', 'y_min', 'x_max', 'y_max']]\n",
    "#     print(  x_min, y_min, x_max, y_max )\n",
    "#     print(df)\n",
    "    \n",
    "    \n",
    "    # Scalling factor\n",
    "#     print((face_bbox['y_max'] - face_bbox['y_min']).values)\n",
    "#     print(face_bbox['x_max'] - face_bbox['x_min'].values)\n",
    "        \n",
    "    y_scale = new_shape[0] / (face_bbox['y_max'] - face_bbox['y_min'])\n",
    "    x_scale = new_shape[1] / (face_bbox['x_max'] - face_bbox['x_min']) \n",
    "#     df[facial_landmarks_x] = df[facial_landmarks_x] / x_scale\n",
    "#     df[facial_landmarks_y] = df[facial_landmarks_y] / y_scale\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:48.906910Z",
     "iopub.status.busy": "2021-02-19T08:46:48.906323Z",
     "iopub.status.idle": "2021-02-19T08:46:48.998389Z",
     "shell.execute_reply": "2021-02-19T08:46:48.998939Z"
    },
    "papermill": {
     "duration": 0.155514,
     "end_time": "2021-02-19T08:46:48.999132",
     "exception": false,
     "start_time": "2021-02-19T08:46:48.843618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 259.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 357.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy \n",
    "\n",
    "train_dataset = VideoFrameDataset(\n",
    "    annotation_df = train_df,\n",
    "    num_segments = 50,\n",
    "    frames_per_segment = 60,\n",
    "    transform = None,\n",
    "    jump_frame = 1,\n",
    "    e2e = False,\n",
    "    meta_df = meta\n",
    ")\n",
    "test_dataset = VideoFrameDataset(\n",
    "    annotation_df = test_df,\n",
    "    num_segments = 30,\n",
    "    frames_per_segment = 60,\n",
    "    transform = None,\n",
    "    jump_frame = 1,\n",
    "    e2e = False,\n",
    "    meta_df = meta\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:49.128308Z",
     "iopub.status.busy": "2021-02-19T08:46:49.127724Z",
     "iopub.status.idle": "2021-02-19T08:46:49.130919Z",
     "shell.execute_reply": "2021-02-19T08:46:49.130524Z"
    },
    "papermill": {
     "duration": 0.07235,
     "end_time": "2021-02-19T08:46:49.131028",
     "exception": false,
     "start_time": "2021-02-19T08:46:49.058678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_optical_flow(batch_landmarks) :\n",
    "    b, l, s = batch_landmarks.shape\n",
    "    optical_flow = np.zeros((b,2*l-2,300,300))\n",
    "    for i_b in range(b) :\n",
    "        landmarks = batch_landmarks[i_b]\n",
    "        dx = np.zeros((l-1,300,300), dtype = np.int8)\n",
    "        dy = np.zeros((l-1,300,300), dtype = np.int8)\n",
    "        diff = (landmarks[1:] - landmarks[:-1]) * 3\n",
    "        for i in range(0, l-1) :\n",
    "            coor = landmarks[i].reshape(2,-1).T\n",
    "            d = diff[i].reshape(2,-1).T\n",
    "            for j in range(0, len(coor)) :\n",
    "                x = int(coor[j][0])\n",
    "                y = int(coor[j][1])\n",
    "                x = 0 if x < 0 else x\n",
    "                y = 0 if y < 0 else y\n",
    "                y = 299 if y > 299 else y\n",
    "                x = 299 if x > 299 else x\n",
    "                dx[i][x][y] = int(d[j][0])\n",
    "                dy[i][x][y] = int(d[j][1])\n",
    "            optical_flow[i_b,:l-1,:,:] = dx\n",
    "            optical_flow[i_b,l-1:,:,:] = dx\n",
    "    return optical_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:49.251767Z",
     "iopub.status.busy": "2021-02-19T08:46:49.251151Z",
     "iopub.status.idle": "2021-02-19T08:46:49.255009Z",
     "shell.execute_reply": "2021-02-19T08:46:49.254598Z"
    },
    "papermill": {
     "duration": 0.066324,
     "end_time": "2021-02-19T08:46:49.255120",
     "exception": false,
     "start_time": "2021-02-19T08:46:49.188796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:49.721843Z",
     "iopub.status.busy": "2021-02-19T08:46:49.721014Z",
     "iopub.status.idle": "2021-02-19T08:46:49.723600Z",
     "shell.execute_reply": "2021-02-19T08:46:49.723977Z"
    },
    "papermill": {
     "duration": 0.409655,
     "end_time": "2021-02-19T08:46:49.724131",
     "exception": false,
     "start_time": "2021-02-19T08:46:49.314476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:49.847184Z",
     "iopub.status.busy": "2021-02-19T08:46:49.846552Z",
     "iopub.status.idle": "2021-02-19T08:46:57.521529Z",
     "shell.execute_reply": "2021-02-19T08:46:57.521045Z"
    },
    "papermill": {
     "duration": 7.739005,
     "end_time": "2021-02-19T08:46:57.521663",
     "exception": false,
     "start_time": "2021-02-19T08:46:49.782658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17e6503467549c889da02e6e13b8c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/170M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spatial_model = models.resnet101(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:57.657949Z",
     "iopub.status.busy": "2021-02-19T08:46:57.656127Z",
     "iopub.status.idle": "2021-02-19T08:46:57.661464Z",
     "shell.execute_reply": "2021-02-19T08:46:57.661871Z"
    },
    "papermill": {
     "duration": 0.076602,
     "end_time": "2021-02-19T08:46:57.662003",
     "exception": false,
     "start_time": "2021-02-19T08:46:57.585401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:46:57.788987Z",
     "iopub.status.busy": "2021-02-19T08:46:57.788152Z",
     "iopub.status.idle": "2021-02-19T08:47:02.092460Z",
     "shell.execute_reply": "2021-02-19T08:47:02.091404Z"
    },
    "papermill": {
     "duration": 4.369574,
     "end_time": "2021-02-19T08:47:02.092625",
     "exception": false,
     "start_time": "2021-02-19T08:46:57.723051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spatial_model.fc = nn.Linear(2048, 3, bias = True)\n",
    "for params in spatial_model.parameters() :\n",
    "    params.requires_grad = False\n",
    "    \n",
    "spatial_model.fc.weight.requires_grad = True\n",
    "spatial_model.fc.bias.requires_grad = True\n",
    "\n",
    "adam_spatial = optim.Adam(\n",
    "    filter(lambda p : p.requires_grad, spatial_model.parameters()), \n",
    "    lr=1e-4\n",
    ")\n",
    "spatial_model = spatial_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:47:02.226105Z",
     "iopub.status.busy": "2021-02-19T08:47:02.225305Z",
     "iopub.status.idle": "2021-02-19T08:47:02.228072Z",
     "shell.execute_reply": "2021-02-19T08:47:02.227661Z"
    },
    "papermill": {
     "duration": 0.075231,
     "end_time": "2021-02-19T08:47:02.228188",
     "exception": false,
     "start_time": "2021-02-19T08:47:02.152957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TemporalModel(nn.Module) :\n",
    "    def __init__(self, in_channels) :\n",
    "        super(TemporalModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = in_channels, \n",
    "            out_channels = 96,\n",
    "            kernel_size = 7,\n",
    "            stride = 2\n",
    "        )\n",
    "        self.norm1 = nn.BatchNorm2d(96)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 96,\n",
    "            out_channels = 256,\n",
    "            kernel_size = 5,\n",
    "            stride = 2\n",
    "        )\n",
    "        self.norm2 = nn.BatchNorm2d(256)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels = 256,\n",
    "            out_channels = 512,\n",
    "            kernel_size = 3,\n",
    "            stride = 1\n",
    "        )\n",
    "        self.norm3 = nn.BatchNorm2d(512)\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels = 512,\n",
    "            out_channels = 512,\n",
    "            kernel_size = 3,\n",
    "            stride = 1\n",
    "        )\n",
    "        self.norm4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels = 512,\n",
    "            out_channels = 512,\n",
    "            kernel_size = 3,\n",
    "            stride = 1\n",
    "        )\n",
    "        self.fc1 = nn.Linear(12800, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 3)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:47:02.363705Z",
     "iopub.status.busy": "2021-02-19T08:47:02.362976Z",
     "iopub.status.idle": "2021-02-19T08:47:02.365901Z",
     "shell.execute_reply": "2021-02-19T08:47:02.365441Z"
    },
    "papermill": {
     "duration": 0.077771,
     "end_time": "2021-02-19T08:47:02.366052",
     "exception": false,
     "start_time": "2021-02-19T08:47:02.288281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_2_stream(spatial, temporal, optim_spatial, optim_temporal, epochs=30) :\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_recall = []\n",
    "    test_recall = []\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for e in range(epochs) :\n",
    "        optim_spatial.zero_grad() \n",
    "        optim_temporal.zero_grad()\n",
    "        running_loss = [0.0, 0.0]\n",
    "        for data in tqdm(trainloader) :\n",
    "            frame, label, landmarks = data\n",
    "            frame = frame.float()\n",
    "            frame = frame.permute(0,3,1,2).to(device)\n",
    "            label = label.long().to(device)\n",
    "            optical_flow = generate_optical_flow(landmarks)\n",
    "            optical_flow = torch.Tensor(optical_flow).to(device)\n",
    "            \n",
    "            spatial_out = spatial(frame)\n",
    "            spatial_out = F.softmax(spatial_out, dim=1)\n",
    "            temporal_out = temporal(optical_flow)\n",
    "            spatial_loss = loss_func(spatial_out, label)\n",
    "            temporal_loss = loss_func(temporal_out, label)\n",
    "            \n",
    "            spatial_loss.backward()\n",
    "            temporal_loss.backward()\n",
    "            optim_spatial.step()\n",
    "            optim_temporal.step()\n",
    "            \n",
    "            running_loss[0] += spatial_loss.item()\n",
    "            running_loss[1] += temporal_loss.item()\n",
    "        train_loss.append(running_loss)\n",
    "        \n",
    "        running_loss = [0.0, 0.0]\n",
    "        count = 0\n",
    "        acc = [[], []]\n",
    "        with torch.no_grad() :\n",
    "            for data in tqdm(testloader) :\n",
    "                frame, label, landmarks = data\n",
    "                label = label.long().to(device)\n",
    "                frame = frame.float().to(device)\n",
    "                frame = frame.permute(0,3,1,2)\n",
    "                optical_flow = generate_optical_flow(landmarks)\n",
    "                optical_flow = torch.Tensor(optical_flow).to(device)\n",
    "\n",
    "                spatial_out = spatial(frame)\n",
    "                spatial_out = F.softmax(spatial_out, dim=1)\n",
    "                temporal_out = temporal(optical_flow)\n",
    "\n",
    "                spatial_loss = loss_func(spatial_out, label)\n",
    "                temporal_loss = loss_func(temporal_out, label)\n",
    "\n",
    "                running_loss[0] += spatial_loss.item()\n",
    "                running_loss[1] += temporal_loss.item()\n",
    "                \n",
    "                spatial_pred = spatial_out.argmax(axis=1)\n",
    "                temporal_pred = temporal_out.argmax(axis=1)\n",
    "                acc[0].append(torch.sum(spatial_pred == label)/len(label)) \n",
    "                acc[1].append(torch.sum(temporal_pred == label)/len(label))\n",
    "        acc0 = torch.mean(torch.Tensor(acc[0]))\n",
    "        acc1 = torch.mean(torch.Tensor(acc[1]))\n",
    "        test_acc.append([acc0, acc1])\n",
    "        test_loss.append(running_loss)\n",
    "    return train_loss, test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:47:02.492117Z",
     "iopub.status.busy": "2021-02-19T08:47:02.491412Z",
     "iopub.status.idle": "2021-02-19T08:47:02.992512Z",
     "shell.execute_reply": "2021-02-19T08:47:02.991945Z"
    },
    "papermill": {
     "duration": 0.566015,
     "end_time": "2021-02-19T08:47:02.992650",
     "exception": false,
     "start_time": "2021-02-19T08:47:02.426635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temporal = TemporalModel(118)\n",
    "adam_temporal = optim.Adam(temporal.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:47:03.141619Z",
     "iopub.status.busy": "2021-02-19T08:47:03.140582Z",
     "iopub.status.idle": "2021-02-19T08:47:03.215848Z",
     "shell.execute_reply": "2021-02-19T08:47:03.215115Z"
    },
    "papermill": {
     "duration": 0.154476,
     "end_time": "2021-02-19T08:47:03.215972",
     "exception": false,
     "start_time": "2021-02-19T08:47:03.061496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temporal = temporal.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T08:47:03.342376Z",
     "iopub.status.busy": "2021-02-19T08:47:03.341815Z",
     "iopub.status.idle": "2021-02-19T13:02:34.088029Z",
     "shell.execute_reply": "2021-02-19T13:02:34.086421Z"
    },
    "papermill": {
     "duration": 15330.811388,
     "end_time": "2021-02-19T13:02:34.088199",
     "exception": false,
     "start_time": "2021-02-19T08:47:03.276811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [22:38<00:00, 75.50s/it]\n",
      "100%|██████████| 6/6 [03:47<00:00, 37.90s/it]\n",
      "100%|██████████| 18/18 [21:57<00:00, 73.19s/it]\n",
      "100%|██████████| 6/6 [03:41<00:00, 36.84s/it]\n",
      "100%|██████████| 18/18 [21:41<00:00, 72.29s/it]\n",
      "100%|██████████| 6/6 [03:40<00:00, 36.67s/it]\n",
      "100%|██████████| 18/18 [21:41<00:00, 72.29s/it]\n",
      "100%|██████████| 6/6 [03:43<00:00, 37.26s/it]\n",
      "100%|██████████| 18/18 [21:47<00:00, 72.61s/it]\n",
      "100%|██████████| 6/6 [03:39<00:00, 36.54s/it]\n",
      "100%|██████████| 18/18 [21:42<00:00, 72.37s/it]\n",
      "100%|██████████| 6/6 [03:41<00:00, 36.88s/it]\n",
      "100%|██████████| 18/18 [21:47<00:00, 72.64s/it]\n",
      "100%|██████████| 6/6 [03:39<00:00, 36.59s/it]\n",
      "100%|██████████| 18/18 [21:42<00:00, 72.38s/it]\n",
      "100%|██████████| 6/6 [03:43<00:00, 37.26s/it]\n",
      "100%|██████████| 18/18 [21:49<00:00, 72.75s/it]\n",
      "100%|██████████| 6/6 [03:39<00:00, 36.57s/it]\n",
      "100%|██████████| 18/18 [21:47<00:00, 72.65s/it]\n",
      "100%|██████████| 6/6 [03:39<00:00, 36.55s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_acc, test_loss = train_2_stream(spatial_model, temporal, adam_spatial, adam_temporal, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T13:02:34.363462Z",
     "iopub.status.busy": "2021-02-19T13:02:34.360043Z",
     "iopub.status.idle": "2021-02-19T13:02:35.624288Z",
     "shell.execute_reply": "2021-02-19T13:02:35.623802Z"
    },
    "papermill": {
     "duration": 1.404637,
     "end_time": "2021-02-19T13:02:35.624441",
     "exception": false,
     "start_time": "2021-02-19T13:02:34.219804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(temporal, './temporal.pt')\n",
    "torch.save(spatial_model , './spatial.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T13:02:35.891284Z",
     "iopub.status.busy": "2021-02-19T13:02:35.890539Z",
     "iopub.status.idle": "2021-02-19T13:02:35.894196Z",
     "shell.execute_reply": "2021-02-19T13:02:35.893727Z"
    },
    "papermill": {
     "duration": 0.13893,
     "end_time": "2021-02-19T13:02:35.894305",
     "exception": false,
     "start_time": "2021-02-19T13:02:35.755375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(torch.Tensor(train_loss), './trainloss.pt')\n",
    "torch.save(torch.Tensor(test_loss), './testloss.pt')\n",
    "torch.save(torch.Tensor(test_acc), './testacc.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15383.381024,
   "end_time": "2021-02-19T13:02:38.192005",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-19T08:46:14.810981",
   "version": "2.2.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06f4ffc47dbf40178256000c3c3425c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f536583545e4ec2a2ee03c3d2f83d47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c3b402221ce04cb897ea9f1eefeb674d",
       "placeholder": "​",
       "style": "IPY_MODEL_863b9c3172d944e3b817d82907786953",
       "value": " 170M/170M [00:05&lt;00:00, 33.3MB/s]"
      }
     },
     "2fb3a7e7fa704c349904f2734ee7994f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_304d95d6f3ee47f88723ad574afa81a2",
       "max": 178728960.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cfb58f0c77b9443ab149b0ea3eb45b88",
       "value": 178728960.0
      }
     },
     "304d95d6f3ee47f88723ad574afa81a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5dc21af9d57442c9b0b4207f812d6d19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_06f4ffc47dbf40178256000c3c3425c0",
       "placeholder": "​",
       "style": "IPY_MODEL_abd911707c4741429d43e94ec18eccf4",
       "value": "100%"
      }
     },
     "863b9c3172d944e3b817d82907786953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "abd911707c4741429d43e94ec18eccf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b96e7e8f82da4556a84b37df3508de06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3b402221ce04cb897ea9f1eefeb674d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cfb58f0c77b9443ab149b0ea3eb45b88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f17e6503467549c889da02e6e13b8c91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5dc21af9d57442c9b0b4207f812d6d19",
        "IPY_MODEL_2fb3a7e7fa704c349904f2734ee7994f",
        "IPY_MODEL_0f536583545e4ec2a2ee03c3d2f83d47"
       ],
       "layout": "IPY_MODEL_b96e7e8f82da4556a84b37df3508de06"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
